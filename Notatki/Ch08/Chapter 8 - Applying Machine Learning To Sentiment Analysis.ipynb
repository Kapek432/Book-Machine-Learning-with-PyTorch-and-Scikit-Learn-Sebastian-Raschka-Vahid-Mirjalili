{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMH3WH9XsgXFZRPcpVvcZi4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter 8 - Applying Machine Learning To Sentiment Analysis"],"metadata":{"id":"dcjkffvM_NeW"}},{"cell_type":"markdown","source":["## Preparing the IMDb movie review data for text processing"],"metadata":{"id":"dEtC3Tu1_YUE"}},{"cell_type":"markdown","source":["### Obtaining the movie review dataset\n","The IMDB movie review set can be downloaded from http://ai.stanford.edu/~amaas/data/sentiment/. After downloading the dataset, decompress the files."],"metadata":{"id":"qsD_jP3K_9n8"}},{"cell_type":"code","source":["import os\n","import sys\n","import tarfile\n","import time\n","import urllib.request\n","\n","source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n","target = 'aclImdb_v1.tar.gz'\n","\n","if os.path.exists(target):\n","    os.remove(target)\n","\n","def reporthook(count, block_size, total_size):\n","    global start_time\n","    if count == 0:\n","        start_time = time.time()\n","        return\n","    duration = time.time() - start_time\n","    progress_size = int(count * block_size)\n","    speed = progress_size / (1024.**2 * duration)\n","    percent = count * block_size * 100. / total_size\n","\n","    sys.stdout.write(f'\\r{int(percent)}% | {progress_size / (1024.**2):.2f} MB '\n","                     f'| {speed:.2f} MB/s | {duration:.2f} sec elapsed')\n","    sys.stdout.flush()\n","\n","\n","if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n","    urllib.request.urlretrieve(source, target, reporthook)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdI4_UlbEmJP","executionInfo":{"status":"ok","timestamp":1739402263747,"user_tz":-60,"elapsed":49438,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"5b801b83-30e7-49ce-c09a-cb24d0fcdf5d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["100% | 80.23 MB | 1.67 MB/s | 47.93 sec elapsed"]}]},{"cell_type":"code","source":["if not os.path.isdir('aclImdb'):\n","\n","    with tarfile.open(target, 'r:gz') as tar:\n","        tar.extractall()"],"metadata":{"id":"Z9ZY3g1lFHeZ","executionInfo":{"status":"ok","timestamp":1739402290118,"user_tz":-60,"elapsed":26373,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing the movie dataset into a more convenient format\n","Having successfully extracted the dataset, we will now assemble the individual text documents from\n","the decompressed download archive into a single CSV file. In the following code section, we will be\n","reading the movie reviews into a pandas DataFrame object"],"metadata":{"id":"a9uqFzPdAUhU"}},{"cell_type":"markdown","source":["To visualize the progress and estimated time until completion, we will use the Python Progress Indicator."],"metadata":{"id":"wLQ7xlOiAegi"}},{"cell_type":"code","source":["!pip install pyprind"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8KTbmwnFd2d","executionInfo":{"status":"ok","timestamp":1739402293218,"user_tz":-60,"elapsed":3104,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"b5a08ec6-e85a-4650-876e-79ba535c3fb6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyprind\n","  Downloading PyPrind-2.11.3-py2.py3-none-any.whl.metadata (1.1 kB)\n","Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n","Installing collected packages: pyprind\n","Successfully installed pyprind-2.11.3\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jI8YWL4d_Jl_","executionInfo":{"status":"ok","timestamp":1739402361289,"user_tz":-60,"elapsed":68076,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"acad1941-0236-45f5-9078-9c3f013acd0a"},"outputs":[{"output_type":"stream","name":"stderr","text":["0% [##############################] 100% | ETA: 00:00:00\n","Total time elapsed: 00:01:07\n"]}],"source":["import pyprind\n","import pandas as pd\n","import os\n","import sys\n","from packaging import version\n","\n","\n","# change the `basepath` to the directory of the\n","# unzipped movie dataset\n","\n","basepath = '/content/aclImdb'\n","\n","labels = {'pos': 1, 'neg': 0}\n","\n","# if the progress bar does not show, change stream=sys.stdout to stream=2\n","pbar = pyprind.ProgBar(50000, stream=2)\n","\n","df = pd.DataFrame()\n","for s in ('test', 'train'):\n","    for l in ('pos', 'neg'):\n","        path = os.path.join(basepath, s, l)\n","        for file in sorted(os.listdir(path)):\n","            with open(os.path.join(path, file),\n","                      'r', encoding='utf-8') as infile:\n","                txt = infile.read()\n","\n","            if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n","                x = pd.DataFrame([[txt, labels[l]]], columns=['review', 'sentiment'])\n","                df = pd.concat([df, x], ignore_index=False)\n","\n","            else:\n","                df = df.append([[txt, labels[l]]],\n","                               ignore_index=True)\n","            pbar.update()\n","df.columns = ['review', 'sentiment']"]},{"cell_type":"code","source":["#Shuffling\n","import numpy as np\n","\n","\n","if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n","    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n","\n","else:\n","    np.random.seed(0)\n","    df = df.reindex(np.random.permutation(df.index))"],"metadata":{"id":"ttmfyWEhMFNb","executionInfo":{"status":"ok","timestamp":1739402361290,"user_tz":-60,"elapsed":6,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["In the preceding code, we first initialized a new progress bar object, pbar, with 50,000 iterations, which\n","was the number of documents we were going to read in. Using the nested for loops, we iterated over\n","the train and test subdirectories in the main aclImdb directory and read the individual text files\n","from the pos and neg subdirectories that we eventually appended to the df pandas DataFrame, together\n","with an integer class label (1 = positive and 0 = negative)."],"metadata":{"id":"06FrL_DHB7YN"}},{"cell_type":"markdown","source":["Since the class labels in the assembled dataset are sorted, we will now shuffle the DataFrame using the\n","permutation function from the np.random submodule—this will be useful for splitting the dataset into\n","training and test datasets in later sections, when we will stream the data from our local drive directly.\n","\n","\n","For our own convenience, we will also store the assembled and shuffled movie review dataset as a\n","CSV file:"],"metadata":{"id":"vfVKt0sYCAws"}},{"cell_type":"code","source":["import numpy as np\n","np.random.seed(0)\n","df = df.reindex(np.random.permutation(df.index))\n","df.to_csv('movie_data.csv', index=False, encoding='utf-8')"],"metadata":{"id":"W2Ux2FeCB74f","executionInfo":{"status":"ok","timestamp":1739402363898,"user_tz":-60,"elapsed":2614,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Since we are going to use this dataset later in this chapter, let’s quickly confirm that we have successfully\n","saved the data in the right format by reading in the CSV and printing an excerpt of the first three\n","examples:\n"],"metadata":{"id":"Z2dM-YuICwFH"}},{"cell_type":"code","source":["df = pd.read_csv('movie_data.csv',encoding='utf-8')\n","df=df.rename(columns={'0' : 'review', '1' : 'sentiment'})\n","df.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"CwX1JYcpCwbf","executionInfo":{"status":"ok","timestamp":1739402367055,"user_tz":-60,"elapsed":3159,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"541bf1ce-ee7a-4a0f-ccb2-dcc11eb093dd"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review  sentiment\n","0  Election is a Chinese mob movie, or triads in ...          1\n","1  I was just watching a Forensic Files marathon ...          0\n","2  Police Story is a stunning series of set piece...          1"],"text/html":["\n","  <div id=\"df-afc9409c-d045-4021-9733-5747899e8200\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Election is a Chinese mob movie, or triads in ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I was just watching a Forensic Files marathon ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Police Story is a stunning series of set piece...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-afc9409c-d045-4021-9733-5747899e8200')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-afc9409c-d045-4021-9733-5747899e8200 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-afc9409c-d045-4021-9733-5747899e8200');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1c5c5a1e-bed6-4c53-a6e6-598c85838bdd\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1c5c5a1e-bed6-4c53-a6e6-598c85838bdd')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1c5c5a1e-bed6-4c53-a6e6-598c85838bdd button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Nothing but the void, a pleasant one for those who have known the eighties, but well, quite boring for those who are not interested in it. NO screenplay in this film, but a hero wandering in an underground New York full of arstists and night clubbers. It is aimless, pointless and naive. But not entirely unpleasant.\",\n          \"Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tones) becomes acquainted and friends with a fourteen-year-old boy from Wisconsin named Pete Logand(Rory Culkin),who has written a book detailing sexual abuse from his parents. To boot,Pete has AIDS and this compels Gabriel further still,since his partner Jess(Bobby Cannavale,good)happens to be a survivor of HIV himself. <br /><br />He also acquaints himself with Pete's guardian,a woman named Donna(Toni Collette,brilliant!)and when Gabriel decides he wants to meet and talk to the two of them in person and goes to Wisconsin,he discovers some secrets he was(naturally)not prepared to find.<br /><br />Based on real events that happened to Armistead Maupin(who co-wrote the screenplay with Terry Anderson)and directed by Patrick Stetner,this film moves a lot faster(90 min.,maybe a few minutes longer)than one might think a movie of this genre would run. That's good in that it keeps the action and storyline lean and clear. It's bad in that it leaves various holes in the plot and doesn't sew-up any of the plot openings or back-story. I'd rather not go into any great detail except to say that,if you are not familiar with Mr.Maupin's works or his personal story,you feel a little bit out of the loop here. Still,the performances by Williams( I would've loved to heard more of his narration,personally),Collette,Cannavale,Culkin and much of the supporting cast(the Waitress at the restaurant Collete's Donna frequents does a great job with what small part she has!)are top-notch and the mood established here--namely,the chilly,lonely dark exteriors of Wisconsin and New York--give a terrific framing for this story. It may have ends that don't tie together particularly well,but it's still a compelling enough story to stick with.\",\n          \"I just cannot believe the low scores for this movie. Probable reason has to do with the low number of votes meaning few people have seen it. This is simply a fantastic movie! There are so many stories inter-wined within but it's not complicated. Each character grows with the movie and we experience with them undergoing life changes. The scenery is simply amazing and the end credits are the best ever in any movie I have seen (just like a Shakespeare play). Yes, it's a little dated (filmed in 1982) but the issues the characters face are very current. It could have been filmed in 2002 without modifications to the story line. Raul Julia is amazing, best role ever in a movie - this is his signature piece. A young Molly Ringwald is excellent as she matures from girl to young woman. Susan Sarandon is perfect as a young carefree woman and John Cassavetes is the force that puts this all together. Do yourself a favor, find this movie, view it & enjoy it. Come back to IMDb and score this movie into the top 250 of all time where it really belongs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjODuP91F_io","executionInfo":{"status":"ok","timestamp":1739402367056,"user_tz":-60,"elapsed":8,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"98060514-e802-4dad-d2df-3ca4c5113fa1"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 2)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## Introducing the bag-of-words model"],"metadata":{"id":"Zc3V4vLNGGGe"}},{"cell_type":"markdown","source":["### Transforming words into feature vectors\n","CountVectorizer takes an array of text data, which can be documents or sentences, and constructs\n","the bag-of-words model for us:"],"metadata":{"id":"zsqN5rkRGWsJ"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","count = CountVectorizer()\n","docs = np.array(['The sun is shining',\n","                 'The weather is sweet',\n","                 'The sun is shining, the weather is sweet, and one and one is two'])\n","bag = count.fit_transform(docs)"],"metadata":{"id":"zv_fJca7GAWD","executionInfo":{"status":"ok","timestamp":1739402368437,"user_tz":-60,"elapsed":1388,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["By calling the fit_transform method on CountVectorizer, we constructed the vocabulary of the\n","bag-of-words model and transformed the following three sentences into sparse feature vectors:\n","\n","• 'The sun is shining'\n","\n","• 'The weather is sweet'\n","\n","• 'The sun is shining, the weather is sweet, and one and one is two'\n","\n","Now, let’s print the contents of the vocabulary to get a better understanding of the underlying concepts:"],"metadata":{"id":"3ls038q8G2a9"}},{"cell_type":"code","source":["print(count.vocabulary_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eyn04bgxG-Q8","executionInfo":{"status":"ok","timestamp":1739402368437,"user_tz":-60,"elapsed":23,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"7353f35f-6af2-4925-e6e1-e01675581a5b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"]}]},{"cell_type":"markdown","source":["As you can see from executing the preceding command, the vocabulary is stored in a Python dictionary\n","that maps the unique words to integer indices. Next, let’s print the feature vectors that we just created:"],"metadata":{"id":"ZJUJOgHsHMIV"}},{"cell_type":"code","source":["print(bag.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KO45KswjHEOc","executionInfo":{"status":"ok","timestamp":1739402368437,"user_tz":-60,"elapsed":22,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"c4cd582a-dae9-43ee-b6e0-1057c3655a64"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 0 1 1 0 1 0 0]\n"," [0 1 0 0 0 1 1 0 1]\n"," [2 3 2 1 1 1 2 1 1]]\n"]}]},{"cell_type":"markdown","source":["### Assessing word relevancy via term frequency-inverse document frequency (TF-IDF)"],"metadata":{"id":"HHFxRtQ9H_ft"}},{"cell_type":"markdown","source":["**Częstotliwość występowania terminu - TF**\n","\n","\n","Mierzy, jak często dane słowo występuje w danym dokumencie. Zakłada się, że im częściej słowo pojawia się w dokumencie, tym jest ważniejsze w tym dokumencie.\n","\n","**Odwrócona częstotliwość dokumentu - IDF**\n","\n","\n","Mierzy, jak ważne jest słowo w całym zbiorze dokumentów. Słowa, które pojawiają się w wielu dokumentach, są uznawane za mniej istotne (często występujące), a te, które pojawiają się w mniej dokumentach, są uznawane za bardziej istotne (rzadsze). Celem IDF jest zmniejszenie wagi słów, które występują zbyt często w zbiorze dokumentów."],"metadata":{"id":"Ry-uFkrjJ2aL"}},{"cell_type":"markdown","source":["The scikit-learn library implements yet another transformer, the TfidfTransformer class, which takes\n","the raw term frequencies from the CountVectorizer class as input and transforms them into tf-idfs:"],"metadata":{"id":"IyfJsJS6KOlb"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf = TfidfTransformer(use_idf=True,\n","                         norm='l2',\n","                         smooth_idf=True)\n","\n","np.set_printoptions(precision=2)\n","print(tfidf.fit_transform(count.fit_transform(docs))\n","      .toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FiOfopbJHRdc","executionInfo":{"status":"ok","timestamp":1739402368437,"user_tz":-60,"elapsed":19,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"15a4e6c8-02f2-4532-dd32-9ce8bd38d462"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n"," [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n"," [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"]}]},{"cell_type":"markdown","source":["As you saw in the previous subsection, the word 'is' had the largest term frequency in the third\n","document, being the most frequently occurring word. However, after transforming the same feature\n","vector into tf-idfs, the word 'is' is now associated with a relatively small tf-idf (0.45) in the third\n","document, since it is also present in the first and second document and thus is unlikely to contain\n","any useful discriminatory information."],"metadata":{"id":"JW3JHryhK2Ac"}},{"cell_type":"markdown","source":["### Cleaning text data"],"metadata":{"id":"_30SP_1dLSv5"}},{"cell_type":"markdown","source":["Let’s display the last 50 characters from the first document in the\n","reshuffled movie review dataset:"],"metadata":{"id":"E5_6cmAlLkRj"}},{"cell_type":"code","source":["df.loc[16,'review'][-50:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"XTl19p6KKfqM","executionInfo":{"status":"ok","timestamp":1739402368437,"user_tz":-60,"elapsed":16,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"20df0016-34ad-4266-912d-cbad64e935ce"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' a marvel to look at and never stops for a second.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["As you can see here, the text contains HTML markup as well as punctuation and other non-letter\n","characters. While HTML markup does not contain many useful semantics, punctuation marks can\n","represent useful, additional information in certain NLP contexts. However, for simplicity, we will now\n","remove all punctuation marks except for emoticon characters, such as :), since those are certainly\n","useful for sentiment analysis."],"metadata":{"id":"23QYOv81MreK"}},{"cell_type":"markdown","source":["To accomplish this task, we will use Python’s regular expression (regex) library, re, as shown here:"],"metadata":{"id":"3M3MCDtBMutx"}},{"cell_type":"code","source":["import re\n","def preprocessor(text):\n","  text = re.sub('<[^>]*>','',text)\n","  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n","\n","  text = (re.sub('[\\W]+',' ',text.lower()) + ' '.join(emoticons).replace('-','')) # usuwa niealfanumeryczne, zmienia na male, dodaje emotikony na koniec\n","  return text\n",""],"metadata":{"id":"Z_7rh7AYMuAa","executionInfo":{"status":"ok","timestamp":1739402368438,"user_tz":-60,"elapsed":15,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Via the first regex, <[^>]*>, in the preceding code section, we tried to remove all of the HTML markup\n","from the movie reviews. After we removed the HTML markup, we used a slightly\n","more complex regex to find emoticons, which we temporarily stored as emoticons. Next, we removed all\n","non-word characters from the text via the regex [\\W]+ and converted the text into lowercase characters."],"metadata":{"id":"66wgMVetNVdf"}},{"cell_type":"markdown","source":["`(?: ...)` tworzy **non-capturing group** w wyrażeniach regularnych.\n","\n"," **Co to oznacza?**  \n","Normalnie nawiasy `()` w wyrażeniach regularnych tworzą **grupy przechwytywania** (**capturing groups**), co oznacza, że dane dopasowanie można później odwołać np. przez `re.match().group(1)`. Jednak dodanie `?:` na początku nawiasów powoduje, że grupa **nie jest przechwytywana**, co oznacza, że nie można jej później użyć jako odwołanie.\n","\n"],"metadata":{"id":"e0XaGzGpOOpp"}},{"cell_type":"code","source":["preprocessor(df.loc[16,'review'][-50:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"dD0v7dhhLoTL","executionInfo":{"status":"ok","timestamp":1739402368438,"user_tz":-60,"elapsed":15,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"46e692d0-eea5-4950-f313-5761be38fdf3"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' a marvel to look at and never stops for a second '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["preprocessor(\"</a>This :) is :( a test :-)!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CErt6LU2P59o","executionInfo":{"status":"ok","timestamp":1739402368438,"user_tz":-60,"elapsed":14,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"3fe2f2cb-284c-4bb3-8ab9-b2f1109ed341"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'this is a test :) :( :)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#implementing to all movie reviews\n","df['review'] = df['review'].apply(preprocessor)"],"metadata":{"id":"cJ7m8U1dP9kH","executionInfo":{"status":"ok","timestamp":1739402386057,"user_tz":-60,"elapsed":17632,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Processing documents into tokens"],"metadata":{"id":"QlUMU2aZQIvN"}},{"cell_type":"markdown","source":["After successfully preparing the movie review dataset, we now need to think about how to split the\n","text corpora into individual elements. One way to tokenize documents is to split them into individual\n","words by splitting the cleaned documents at their whitespace characters:"],"metadata":{"id":"0iX6OfkSQgQJ"}},{"cell_type":"code","source":["def tokenizer(text):\n","  return text.split()\n","tokenizer('runners like running and thus they run')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVX0pnEXQG9X","executionInfo":{"status":"ok","timestamp":1739402386058,"user_tz":-60,"elapsed":4,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"a12c541f-5e0d-478a-a7cf-ddff7309f93a"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["In the context of tokenization, another useful technique is word stemming, which is the process of\n","transforming a word into its root form. It allows us to map related words to the same stem. The Natural Language Toolkit for Python implements the Porter stemming algorithm, which we will use in the following\n","code section."],"metadata":{"id":"Dq3Wrza5QuS3"}},{"cell_type":"code","source":["from nltk.stem.porter import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","def tokenizer_porter(text):\n","  return [porter.stem(word) for word in text.split()]\n","\n","tokenizer_porter('my words is running and it has no sense at all sleeping')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPp-A4UGQu0H","executionInfo":{"status":"ok","timestamp":1739402388719,"user_tz":-60,"elapsed":2664,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"0293a7b0-73dd-4fb4-a666-1e03d35f6fe9"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['my',\n"," 'word',\n"," 'is',\n"," 'run',\n"," 'and',\n"," 'it',\n"," 'ha',\n"," 'no',\n"," 'sens',\n"," 'at',\n"," 'all',\n"," 'sleep']"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["Before we jump into the next section, where we will train a machine learning model using the bagof-\n","words model, let’s briefly talk about another useful topic called stop word removal.\n","\n","Stop words\n","are simply those words that are extremely common in all sorts of texts and probably bear no (or only\n","a little) useful information that can be used to distinguish between different classes of documents.\n","Examples of stop words are is, and, has, and like.\n","\n","Removing stop words can be useful if we are working\n","with raw or normalized term frequencies rather than tf-idfs, which already downweight the frequently\n","occurring words.\n","\n","\n","To remove stop words from the movie reviews, we will use the set of 127 English stop words that is\n","available from the NLTK library, which can be obtained by calling the nltk.download function:"],"metadata":{"id":"WJgsJDASSS_4"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXis__NDRZJZ","executionInfo":{"status":"ok","timestamp":1739402389130,"user_tz":-60,"elapsed":414,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"d44e42ec-193f-4aea-9243-cfba0cebee37"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","\n","[w for w in tokenizer_porter('a runner likes running and runs a lot')\n","if w not in stop]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnicLH1TSacI","executionInfo":{"status":"ok","timestamp":1739402389131,"user_tz":-60,"elapsed":12,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"8d5a8a1d-9118-496a-d090-9751318e1ea1"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['runner', 'like', 'run', 'run', 'lot']"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## Training a logistic regression model for document classification"],"metadata":{"id":"xbCkEVCqS4ge"}},{"cell_type":"code","source":["# 25,000 for training and 25,000 for testing\n","X_train = df.loc[:25000, 'review'].values\n","y_train = df.loc[:25000, 'sentiment'].values\n","X_test = df.loc[25000:, 'review'].values\n","y_test = df.loc[25000:, 'sentiment'].values"],"metadata":{"id":"WctEZ7WpS2QW","executionInfo":{"status":"ok","timestamp":1739402389131,"user_tz":-60,"elapsed":10,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["`GridSearchCV` object to find the optimal set of parameters for our logistic regression\n","model using 5-fold stratified cross-validation:"],"metadata":{"id":"OmJsGSYHTh_E"}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import GridSearchCV\n","\n","tfidf = TfidfVectorizer(strip_accents=None,\n","                        lowercase=False,\n","                        preprocessor=None)\n","\n","\"\"\"\n","param_grid = [{'vect__ngram_range': [(1, 1)],\n","               'vect__stop_words': [stop, None],\n","               'vect__tokenizer': [tokenizer, tokenizer_porter],\n","               'clf__penalty': ['l1', 'l2'],\n","               'clf__C': [1.0, 10.0, 100.0]},\n","              {'vect__ngram_range': [(1, 1)],\n","               'vect__stop_words': [stop, None],\n","               'vect__tokenizer': [tokenizer, tokenizer_porter],\n","               'vect__use_idf':[False],\n","               'vect__norm':[None],\n","               'clf__penalty': ['l1', 'l2'],\n","               'clf__C': [1.0, 10.0, 100.0]},\n","              ]\n","\"\"\"\n","\n","small_param_grid = [{'vect__ngram_range': [(1, 1)],\n","                     'vect__stop_words': [None],\n","                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n","                     'clf__penalty': ['l2'],\n","                     'clf__C': [1.0, 10.0]},\n","                    {'vect__ngram_range': [(1, 1)],\n","                     'vect__stop_words': [stop, None],\n","                     'vect__tokenizer': [tokenizer],\n","                     'vect__use_idf':[False],\n","                     'vect__norm':[None],\n","                     'clf__penalty': ['l2'],\n","                  'clf__C': [1.0, 10.0]},\n","              ]\n","\n","lr_tfidf = Pipeline([('vect', tfidf),\n","                     ('clf', LogisticRegression(solver='liblinear'))])\n","\n","gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n","                           scoring='accuracy',\n","                           cv=5,\n","                           verbose=1,\n","                           n_jobs=-1)"],"metadata":{"id":"a-591jT-TdvZ","executionInfo":{"status":"ok","timestamp":1739402389131,"user_tz":-60,"elapsed":9,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["gs_lr_tfidf.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":262},"id":"VKEZ-wpHT_Mx","executionInfo":{"status":"ok","timestamp":1739403612873,"user_tz":-60,"elapsed":1223751,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"68d3e06d-20a7-45b6-b67f-16d58acc27e7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5,\n","             estimator=Pipeline(steps=[('vect',\n","                                        TfidfVectorizer(lowercase=False)),\n","                                       ('clf',\n","                                        LogisticRegression(solver='liblinear'))]),\n","             n_jobs=-1,\n","             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n","                          'vect__ngram_range': [(1, 1)],\n","                          'vect__stop_words': [None],\n","                          'vect__tokenizer': [<function tokenizer at 0x7fb42928a200>,\n","                                              <function tokenizer_porter at 0x7fb42928a3e0...\n","                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n","                                                'our', 'ours', 'ourselves',\n","                                                'you', \"you're\", \"you've\",\n","                                                \"you'll\", \"you'd\", 'your',\n","                                                'yours', 'yourself',\n","                                                'yourselves', 'he', 'him',\n","                                                'his', 'himself', 'she',\n","                                                \"she's\", 'her', 'hers',\n","                                                'herself', 'it', \"it's\", 'its',\n","                                                'itself', ...],\n","                                               None],\n","                          'vect__tokenizer': [<function tokenizer at 0x7fb42928a200>],\n","                          'vect__use_idf': [False]}],\n","             scoring='accuracy', verbose=1)"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n","             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n","                                        TfidfVectorizer(lowercase=False)),\n","                                       (&#x27;clf&#x27;,\n","                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n","             n_jobs=-1,\n","             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n","                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n","                          &#x27;vect__stop_words&#x27;: [None],\n","                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7fb42928a200&gt;,\n","                                              &lt;function tokenizer_porter at 0x7fb42928a3e0...\n","                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n","                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n","                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n","                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n","                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n","                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n","                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n","                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n","                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n","                                                &#x27;itself&#x27;, ...],\n","                                               None],\n","                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7fb42928a200&gt;],\n","                          &#x27;vect__use_idf&#x27;: [False]}],\n","             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n","             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n","                                        TfidfVectorizer(lowercase=False)),\n","                                       (&#x27;clf&#x27;,\n","                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n","             n_jobs=-1,\n","             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n","                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n","                          &#x27;vect__stop_words&#x27;: [None],\n","                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7fb42928a200&gt;,\n","                                              &lt;function tokenizer_porter at 0x7fb42928a3e0...\n","                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n","                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n","                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n","                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n","                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n","                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n","                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n","                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n","                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n","                                                &#x27;itself&#x27;, ...],\n","                                               None],\n","                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7fb42928a200&gt;],\n","                          &#x27;vect__use_idf&#x27;: [False]}],\n","             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n","                 TfidfVectorizer(lowercase=False,\n","                                 tokenizer=&lt;function tokenizer at 0x7fb42928a200&gt;)),\n","                (&#x27;clf&#x27;, LogisticRegression(C=10.0, solver=&#x27;liblinear&#x27;))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(lowercase=False,\n","                tokenizer=&lt;function tokenizer at 0x7fb42928a200&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=10.0, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["print(f'Best parameter set: {gs_lr_tfidf.best_params_}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XaOArLJuUn4E","executionInfo":{"status":"ok","timestamp":1739403612873,"user_tz":-60,"elapsed":9,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"74273147-3617-4c95-aa12-96845dc02f78"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7fb42928a200>}\n"]}]},{"cell_type":"markdown","source":["Using the best model from this grid search, let’s print the average 5-fold cross-validation accuracy\n","scores on the training dataset and the classification accuracy on the test dataset:"],"metadata":{"id":"mxzzZ2NAVCjq"}},{"cell_type":"code","source":["print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')\n","clf = gs_lr_tfidf.best_estimator_\n","print(f'Test Accuracy: {clf.score(X_test, y_test)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVvZxjaCVFu4","executionInfo":{"status":"ok","timestamp":1739403615588,"user_tz":-60,"elapsed":2723,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"c1ea7551-1cf4-417a-e520-ecbbbc105fb6"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["CV Accuracy: 0.899\n","Test Accuracy: 0.89608\n"]}]},{"cell_type":"markdown","source":["## Working with bigger data - online algorithms and out-of-core learning\n","Since not everyone has access to supercomputer facilities, we will now apply a technique called out-ofcore\n","learning, which allows us to work with such large datasets by fitting the classifier incrementally\n","on smaller batches of a dataset."],"metadata":{"id":"3EcmUgUfVXvK"}},{"cell_type":"markdown","source":["First, we will define a tokenizer function that cleans the unprocessed text data from the movie_data.\n","csv file that we constructed at the beginning of this chapter and separates it into word tokens while\n","removing stop words:"],"metadata":{"id":"x_6XbKpHWR2Y"}},{"cell_type":"code","source":["import numpy as np\n","import re\n","from nltk.corpus import stopwords\n","\n","\n","# The `stop` is defined as earlier in this chapter\n","# Added it here for convenience, so that this section\n","# can be run as standalone without executing prior code\n","# in the directory\n","stop = stopwords.words('english')\n","\n","\n","def tokenizer(text):\n","    text = re.sub('<[^>]*>', '', text)\n","    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n","    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n","        ' '.join(emoticons).replace('-', '')\n","    tokenized = [w for w in text.split() if w not in stop]\n","    return tokenized\n","\n","\n"],"metadata":{"id":"haEKnDN7Vkod","executionInfo":{"status":"ok","timestamp":1739403615589,"user_tz":-60,"elapsed":13,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Next, we will define a generator function, stream_docs, that reads in and returns one document at\n","a time:"],"metadata":{"id":"dyD9ZMuFWa03"}},{"cell_type":"code","source":["def stream_docs(path):\n","    with open(path, 'r', encoding='utf-8') as csv:\n","        next(csv)  # skip header\n","        for line in csv:\n","            text, label = line[:-3], int(line[-2])\n","            yield text, label"],"metadata":{"id":"d_6fb2r1WeJ_","executionInfo":{"status":"ok","timestamp":1739403615589,"user_tz":-60,"elapsed":12,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["next(stream_docs(path='movie_data.csv'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFcgJ1c5WiyN","executionInfo":{"status":"ok","timestamp":1739403615589,"user_tz":-60,"elapsed":12,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"11fcdc72-7ded-4ffa-e7f1-097181990744"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('\"Election is a Chinese mob movie, or triads in this case. Every two years an election is held to decide on a new leader, and at first it seems a toss up between Big D (Tony Leung Ka Fai, or as I know him, \"\"The Other Tony Leung\"\") and Lok (Simon Yam, who was Judge in Full Contact!). Though once Lok wins, Big D refuses to accept the choice and goes to whatever lengths he can to secure recognition as the new leader. Unlike any other Asian film I watch featuring gangsters, this one is not an action movie. It has its bloody moments, when necessary, as in Goodfellas, but it\\'s basically just a really effective drama. There are a lot of characters, which is really hard to keep track of, but I think that plays into the craziness of it all a bit. A 100-year-old baton, which is the symbol of power I mentioned before, changes hands several times before things settle down. And though it may appear that the film ends at the 65 or 70-minute mark, there are still a couple big surprises waiting. Simon Yam was my favorite character here and sort of anchors the picture.<br /><br />Election was quite the award winner at last year\\'s Hong Kong Film Awards, winning for best actor (Tony Leung), best picture, best director (Johnny To, who did Heroic Trio!!), and best screenplay. It also had nominations for cinematography, editing, film score (which I loved), and three more acting performances (including Yam).\"',\n"," 1)"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["We will now define a function, get_minibatch, that will take a document stream from the stream_docs\n","function and return a particular number of documents specified by the size parameter:"],"metadata":{"id":"omQIFac9XROb"}},{"cell_type":"code","source":["def get_minibatch(doc_stream, size):\n","    docs, y = [], []\n","    try:\n","        for _ in range(size):\n","            text, label = next(doc_stream)\n","            docs.append(text)\n","            y.append(label)\n","    except StopIteration:\n","        return None, None\n","    return docs, y"],"metadata":{"id":"ZSP39USUXUMg","executionInfo":{"status":"ok","timestamp":1739403615589,"user_tz":-60,"elapsed":11,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["Unfortunately, we can’t use CountVectorizer for out-of-core learning since it requires holding the\n","complete vocabulary in memory. Also, TfidfVectorizer needs to keep all the feature vectors of the\n","training dataset in memory to calculate the inverse document frequencies. However, another useful\n","vectorizer for text processing implemented in scikit-learn is HashingVectorizer. HashingVectorizer\n","is data-independent and makes use of the hashing trick"],"metadata":{"id":"UJwxkV64XkNg"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.linear_model import SGDClassifier\n","\n","vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None,tokenizer=tokenizer)\n","\n","clf = SGDClassifier(loss='log_loss', random_state=1)\n","doc_stream = stream_docs(path='movie_data.csv')"],"metadata":{"id":"-avtAOo2Xk3B","executionInfo":{"status":"ok","timestamp":1739403803072,"user_tz":-60,"elapsed":378,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["Using the preceding code, we initialized HashingVectorizer with our tokenizer function and set the\n","number of features to 2**21. Furthermore, we reinitialized a logistic regression classifier by setting\n","the loss parameter of SGDClassifier to 'log'. Note that by choosing a large number of features in\n","HashingVectorizer, we reduce the chance of causing hash collisions, but we also increase the number\n","of coefficients in our logistic regression model."],"metadata":{"id":"TVc09YkwiIJC"}},{"cell_type":"markdown","source":["Now comes the really interesting part—having set up all the complementary functions, we can start\n","the out-of-core learning using the following code:"],"metadata":{"id":"vtZW1RsxiN8-"}},{"cell_type":"code","source":["import pyprind\n","pbar = pyprind.ProgBar(45)\n","classes = np.array([0,1])\n","for _ in range(45):\n","  X_train, y_train = get_minibatch(doc_stream, size=1000)\n","  if not X_train:\n","    break\n","  X_train = vect.transform(X_train)\n","  clf.partial_fit(X_train, y_train, classes=classes)\n","  pbar.update()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9Bqx9JpiRDS","executionInfo":{"status":"ok","timestamp":1739403849742,"user_tz":-60,"elapsed":45341,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"e40b565a-18a5-4c96-dee3-9d6d665cd1fc"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["0% [##############################] 100% | ETA: 00:00:00\n","Total time elapsed: 00:00:45\n"]}]},{"cell_type":"markdown","source":["Again, we made use of the PyPrind package to estimate the progress of our learning algorithm. We\n","initialized the progress bar object with 45 iterations and, in the following for loop, we iterated over\n","45 mini-batches of documents where each mini-batch consists of 1,000 documents. Having completed\n","the incremental learning process, we will use the last 5,000 documents to evaluate the performance\n","of our model:"],"metadata":{"id":"hvkb4Qmwipif"}},{"cell_type":"code","source":["X_test, y_test = get_minibatch(doc_stream, size=5000)\n","X_test = vect.transform(X_test)\n","print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIJfIMJ_i-B0","executionInfo":{"status":"ok","timestamp":1739403870036,"user_tz":-60,"elapsed":5753,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"27d8f473-26b9-4e13-c7cf-a28d9f74154e"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.866\n"]}]},{"cell_type":"markdown","source":["As you can see, the accuracy of the model is approximately 87 percent, slightly below the accuracy\n","that we achieved in the previous section using the grid search for hyperparameter tuning. However,\n","out-of-core learning is very memory efficient, and it took less than a minute to complete.\n","\n","O WIELE SZYBSZE"],"metadata":{"id":"AsFGew8mjRIW"}},{"cell_type":"markdown","source":["Finally, we can use the last 5,000 documents to update our model:"],"metadata":{"id":"gbS0ExVrjWdW"}},{"cell_type":"code","source":["clf = clf.partial_fit(X_test, y_test)"],"metadata":{"id":"NO8mhHWyjKbn","executionInfo":{"status":"ok","timestamp":1739403925146,"user_tz":-60,"elapsed":406,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["## Topic modeling with latent Dirichlet allocation\n","\n","Popularna metoda modelowania tematów (topic modeling), która pozwala automatycznie odkrywać ukryte tematy w zbiorze dokumentów tekstowych. Jest to probabilistyczny model generatywny, który zakłada, że każdy dokument składa się z mieszaniny tematów, a każdy temat to zbiór powiązanych słów."],"metadata":{"id":"eVsVxAhSjvkP"}},{"cell_type":"markdown","source":["### Decomposing text documents with LDA\n","\n","LDA is a generative probabilistic model that tries to find groups of words that appear frequently together\n","across different documents. These frequently appearing words represent our topics, assuming\n","that each document is a mixture of different words. The input to an LDA is the bag-of-words model\n","that we discussed earlier in this chapter.\n","\n","Given a bag-of-words matrix as input, LDA decomposes it into two new matrices:\n","\n","• A document-to-topic matrix\n","\n","• A word-to-topic matrix\n","\n","LDA decomposes the bag-of-words matrix in such a way that if we multiply those two matrices together,\n","we will be able to reproduce the input, the bag-of-words matrix, with the lowest possible error.\n","In practice, we are interested in those topics that LDA found in the bag-of-words matrix. The only\n","downside may be that we must define the number of topics beforehand—the number of topics is a\n","hyperparameter of LDA that has to be specified manually."],"metadata":{"id":"s32IJG88keMx"}},{"cell_type":"markdown","source":["### LDA with scikit-learn"],"metadata":{"id":"nCjYOPRIkw1W"}},{"cell_type":"markdown","source":["In this subsection, we will use the LatentDirichletAllocation class implemented in scikit-learn to\n","decompose the movie review dataset and categorize it into different topics. In the following example,\n","we will restrict the analysis to 10 different topics"],"metadata":{"id":"pDoLRQD7lZco"}},{"cell_type":"markdown","source":["First, we are going to load the dataset into a pandas DataFrame using the local movie_data.csv file of\n","the movie reviews that we created at the beginning of this chapter:"],"metadata":{"id":"YcTsHU5Ildgk"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('movie_data.csv', encoding='utf-8')\n","\n","df = df.rename(columns={'0' : 'review', '1' : 'sentiment'})\n","df.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"S4QsEnekjZMR","executionInfo":{"status":"ok","timestamp":1739404521941,"user_tz":-60,"elapsed":2132,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"21668808-9c78-423a-e19e-1ed62e57bb8e"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review  sentiment\n","0  Election is a Chinese mob movie, or triads in ...          1\n","1  I was just watching a Forensic Files marathon ...          0\n","2  Police Story is a stunning series of set piece...          1"],"text/html":["\n","  <div id=\"df-68ba9876-ee32-458b-b54f-a5bd5b822099\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Election is a Chinese mob movie, or triads in ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I was just watching a Forensic Files marathon ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Police Story is a stunning series of set piece...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68ba9876-ee32-458b-b54f-a5bd5b822099')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-68ba9876-ee32-458b-b54f-a5bd5b822099 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-68ba9876-ee32-458b-b54f-a5bd5b822099');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-57d6396d-1733-4f41-8439-0c4812eccd62\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-57d6396d-1733-4f41-8439-0c4812eccd62')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-57d6396d-1733-4f41-8439-0c4812eccd62 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Nothing but the void, a pleasant one for those who have known the eighties, but well, quite boring for those who are not interested in it. NO screenplay in this film, but a hero wandering in an underground New York full of arstists and night clubbers. It is aimless, pointless and naive. But not entirely unpleasant.\",\n          \"Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tones) becomes acquainted and friends with a fourteen-year-old boy from Wisconsin named Pete Logand(Rory Culkin),who has written a book detailing sexual abuse from his parents. To boot,Pete has AIDS and this compels Gabriel further still,since his partner Jess(Bobby Cannavale,good)happens to be a survivor of HIV himself. <br /><br />He also acquaints himself with Pete's guardian,a woman named Donna(Toni Collette,brilliant!)and when Gabriel decides he wants to meet and talk to the two of them in person and goes to Wisconsin,he discovers some secrets he was(naturally)not prepared to find.<br /><br />Based on real events that happened to Armistead Maupin(who co-wrote the screenplay with Terry Anderson)and directed by Patrick Stetner,this film moves a lot faster(90 min.,maybe a few minutes longer)than one might think a movie of this genre would run. That's good in that it keeps the action and storyline lean and clear. It's bad in that it leaves various holes in the plot and doesn't sew-up any of the plot openings or back-story. I'd rather not go into any great detail except to say that,if you are not familiar with Mr.Maupin's works or his personal story,you feel a little bit out of the loop here. Still,the performances by Williams( I would've loved to heard more of his narration,personally),Collette,Cannavale,Culkin and much of the supporting cast(the Waitress at the restaurant Collete's Donna frequents does a great job with what small part she has!)are top-notch and the mood established here--namely,the chilly,lonely dark exteriors of Wisconsin and New York--give a terrific framing for this story. It may have ends that don't tie together particularly well,but it's still a compelling enough story to stick with.\",\n          \"I just cannot believe the low scores for this movie. Probable reason has to do with the low number of votes meaning few people have seen it. This is simply a fantastic movie! There are so many stories inter-wined within but it's not complicated. Each character grows with the movie and we experience with them undergoing life changes. The scenery is simply amazing and the end credits are the best ever in any movie I have seen (just like a Shakespeare play). Yes, it's a little dated (filmed in 1982) but the issues the characters face are very current. It could have been filmed in 2002 without modifications to the story line. Raul Julia is amazing, best role ever in a movie - this is his signature piece. A young Molly Ringwald is excellent as she matures from girl to young woman. Susan Sarandon is perfect as a young carefree woman and John Cassavetes is the force that puts this all together. Do yourself a favor, find this movie, view it & enjoy it. Come back to IMDb and score this movie into the top 250 of all time where it really belongs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["Next, we are going to use the already familiar CountVectorizer to create the bag-of-words matrix as\n","input to the LDA.\n","For convenience, we will use scikit-learn’s built-in English stop word library via stop_words='english':"],"metadata":{"id":"A3FfFKTFmIjZ"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","count = CountVectorizer(stop_words='english',\n","                        max_df=.1,\n","                        max_features=5000)\n","X = count.fit_transform(df['review'].values)"],"metadata":{"id":"nVjaoYj4lqeG","executionInfo":{"status":"ok","timestamp":1739404830638,"user_tz":-60,"elapsed":10310,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["Notice that we set the maximum document frequency of words to be considered to 10 percent (max_\n","df=.1) to exclude words that occur too frequently across documents. The rationale behind the removal\n","of frequently occurring words is that these might be common words appearing across all documents\n","that are, therefore, less likely to be associated with a specific topic category of a given document.\n","Also, we limited the number of words to be considered to the most frequently occurring 5,000 words\n","(max_features=5000), to limit the dimensionality of this dataset to improve the inference performed\n","by LDA. However, both max_df=.1 and max_features=5000 are hyperparameter values chosen arbitrarily."],"metadata":{"id":"SLpL98Zlmm7L"}},{"cell_type":"markdown","source":["The following code example demonstrates how to fit a LatentDirichletAllocation estimator to the\n","bag-of-words matrix and infer the 10 different topics from the documents"],"metadata":{"id":"PLsSzADvmsSD"}},{"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","lda = LatentDirichletAllocation(n_components=10,\n","                                random_state=123,\n","                                learning_method='batch')\n","X_topics = lda.fit_transform(X)"],"metadata":{"id":"81sjaWfTmmU6","executionInfo":{"status":"ok","timestamp":1739405252933,"user_tz":-60,"elapsed":404599,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["The following code example demonstrates how to fit a LatentDirichletAllocation estimator to the\n","bag-of-words matrix and infer the 10 different topics from the documents.\n","\n","After fitting the LDA, we now have access to the components_ attribute of the lda instance, which stores\n","a matrix containing the word importance (here, 5000) for each of the 10 topics in increasing order:"],"metadata":{"id":"4bw3BS09m7KF"}},{"cell_type":"code","source":["lda.components_.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vdZlTrzmxsU","executionInfo":{"status":"ok","timestamp":1739405265627,"user_tz":-60,"elapsed":375,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"c29da220-c855-4cb6-fcc7-5168de731567"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 5000)"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["To analyze the results, let’s print the five most important words for each of the 10 topics. Note that the\n","word importance values are ranked in increasing order. Thus, to print the top five words, we need to\n","sort the topic array in reverse order:"],"metadata":{"id":"FUelYm2YnM6A"}},{"cell_type":"code","source":["n_top_words = 5\n","feature_names = count.get_feature_names_out()\n","\n","for topic_idx, topic in enumerate(lda.components_):\n","  print(f\"Topic {topic_idx + 1}:\")\n","  print(\" \".join([feature_names[i] for i in topic.argsort()\n","  [:-n_top_words - 1:-1]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmR8oGBNnZrt","executionInfo":{"status":"ok","timestamp":1739405268323,"user_tz":-60,"elapsed":415,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"a4b44d2d-ec0d-4d99-b62f-5bb6bf6f4a37"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 1:\n","worst minutes awful script stupid\n","Topic 2:\n","family mother father children girl\n","Topic 3:\n","american war dvd music history\n","Topic 4:\n","human audience cinema art feel\n","Topic 5:\n","police guy car dead murder\n","Topic 6:\n","horror house sex blood gore\n","Topic 7:\n","role performance comedy actor performances\n","Topic 8:\n","series episode episodes tv season\n","Topic 9:\n","book version original effects read\n","Topic 10:\n","action fight guy fun guys\n"]}]},{"cell_type":"markdown","source":["Based on reading the five most important words for each topic, you may guess that the LDA identified\n","the following topics:\n","\n","1. Generally bad movies (not really a topic category)\n","2. Movies about families\n","3. War movies\n","4. Art movies\n","5. Crime movies\n","6. Horror movies\n","7. Comedy movie reviews\n","8. Movies somehow related to TV shows\n","9. Movies based on books\n","10. Action movies\n","\n","To confirm that the categories make sense based on the reviews, let’s plot three movies from the horror\n","movie category (horror movies belong to category 6 at index position 5):"],"metadata":{"id":"dY2HUoQSn6ts"}},{"cell_type":"code","source":["horror = X_topics[:, 5].argsort()[::-1]\n","\n","for iter_idx, movie_idx in enumerate(horror[:3]):\n","    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n","    print(df['review'][movie_idx][:300], '...')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kN0DJHwen8q2","executionInfo":{"status":"ok","timestamp":1739405292641,"user_tz":-60,"elapsed":401,"user":{"displayName":"Kacper Lipiec","userId":"10574009641269018885"}},"outputId":"c65a6597-a42a-4a6d-e7a2-c7e8178c04a9"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Horror movie #1:\n","Emilio Miraglia's first Giallo feature, The Night Evelyn Came Out of the Grave, was a great combination of Giallo and Gothic horror - and this second film is even better! We've got more of the Giallo side of the equation this time around, although Miraglia doesn't lose the Gothic horror stylings tha ...\n","\n","Horror movie #2:\n","This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n","\n","Horror movie #3:\n","This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"]}]}]}